{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Распознавание_слов_из_фразы.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gs4j3DSoMnep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# перезагрузить ноутбук\n",
        "!kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_w4tQsIFMsc0",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrSoPHm55snj",
        "colab_type": "text"
      },
      "source": [
        "# Определяет слова из фразы. \n",
        "Загружаем фразу и путь к ней из директория Words_from_streams\n",
        "\n",
        "Если слово из фразы - одно из списка: 'yes','no','up','down','left','right','stop','go','on','off' - программа должна его опознать: \n",
        "\n",
        "My prediction for 1_no_bed_bird.wav is...\n",
        "no\n",
        "\n",
        "В противном случае программа должна сказать:\n",
        "\n",
        "My prediction for 3_no_bed_bird.wav is...\n",
        "UNKNOWN\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "Используется модель '/content/Simple-Speech-Command-Algorithm/models','model1_dr0.25_lr0.1_ra0.hdf5'\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW2GwtmLNkP2",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sj7ERdkwgQ0n",
        "colab_type": "text"
      },
      "source": [
        "Копирование файлов с Git на локальный диск виртуальной машины - вставка ссылки с гитхаба: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOLXvPzApR-o",
        "colab_type": "code",
        "outputId": "58e4703c-aff7-43ad-fde3-97907b1b826d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!git clone https://github.com/smartsinovich/Simple-Audio-Recognition.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Simple-Audio-Recognition'...\n",
            "remote: Enumerating objects: 45, done.\u001b[K\n",
            "remote: Counting objects:   2% (1/45)\u001b[K\rremote: Counting objects:   4% (2/45)\u001b[K\rremote: Counting objects:   6% (3/45)\u001b[K\rremote: Counting objects:   8% (4/45)\u001b[K\rremote: Counting objects:  11% (5/45)\u001b[K\rremote: Counting objects:  13% (6/45)\u001b[K\rremote: Counting objects:  15% (7/45)\u001b[K\rremote: Counting objects:  17% (8/45)\u001b[K\rremote: Counting objects:  20% (9/45)\u001b[K\rremote: Counting objects:  22% (10/45)\u001b[K\rremote: Counting objects:  24% (11/45)\u001b[K\rremote: Counting objects:  26% (12/45)\u001b[K\rremote: Counting objects:  28% (13/45)\u001b[K\rremote: Counting objects:  31% (14/45)\u001b[K\rremote: Counting objects:  33% (15/45)\u001b[K\rremote: Counting objects:  35% (16/45)\u001b[K\rremote: Counting objects:  37% (17/45)\u001b[K\rremote: Counting objects:  40% (18/45)\u001b[K\rremote: Counting objects:  42% (19/45)\u001b[K\rremote: Counting objects:  44% (20/45)\u001b[K\rremote: Counting objects:  46% (21/45)\u001b[K\rremote: Counting objects:  48% (22/45)\u001b[K\rremote: Counting objects:  51% (23/45)\u001b[K\rremote: Counting objects:  53% (24/45)\u001b[K\rremote: Counting objects:  55% (25/45)\u001b[K\rremote: Counting objects:  57% (26/45)\u001b[K\rremote: Counting objects:  60% (27/45)\u001b[K\rremote: Counting objects:  62% (28/45)\u001b[K\rremote: Counting objects:  64% (29/45)\u001b[K\rremote: Counting objects:  66% (30/45)\u001b[K\rremote: Counting objects:  68% (31/45)\u001b[K\rremote: Counting objects:  71% (32/45)\u001b[K\rremote: Counting objects:  73% (33/45)\u001b[K\rremote: Counting objects:  75% (34/45)\u001b[K\rremote: Counting objects:  77% (35/45)\u001b[K\rremote: Counting objects:  80% (36/45)\u001b[K\rremote: Counting objects:  82% (37/45)\u001b[K\rremote: Counting objects:  84% (38/45)\u001b[K\rremote: Counting objects:  86% (39/45)\u001b[K\rremote: Counting objects:  88% (40/45)\u001b[K\rremote: Counting objects:  91% (41/45)\u001b[K\rremote: Counting objects:  93% (42/45)\u001b[K\rremote: Counting objects:  95% (43/45)\u001b[K\rremote: Counting objects:  97% (44/45)\u001b[K\rremote: Counting objects: 100% (45/45)\u001b[K\rremote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects:   2% (1/45)\u001b[K\rremote: Compressing objects:   4% (2/45)\u001b[K\rremote: Compressing objects:   6% (3/45)\u001b[K\rremote: Compressing objects:   8% (4/45)\u001b[K\rremote: Compressing objects:  11% (5/45)\u001b[K\rremote: Compressing objects:  13% (6/45)\u001b[K\rremote: Compressing objects:  15% (7/45)\u001b[K\rremote: Compressing objects:  17% (8/45)\u001b[K\rremote: Compressing objects:  20% (9/45)\u001b[K\rremote: Compressing objects:  22% (10/45)\u001b[K\rremote: Compressing objects:  24% (11/45)\u001b[K\rremote: Compressing objects:  26% (12/45)\u001b[K\rremote: Compressing objects:  28% (13/45)\u001b[K\rremote: Compressing objects:  31% (14/45)\u001b[K\rremote: Compressing objects:  33% (15/45)\u001b[K\rremote: Compressing objects:  35% (16/45)\u001b[K\rremote: Compressing objects:  37% (17/45)\u001b[K\rremote: Compressing objects:  40% (18/45)\u001b[K\rremote: Compressing objects:  42% (19/45)\u001b[K\rremote: Compressing objects:  44% (20/45)\u001b[K\rremote: Compressing objects:  46% (21/45)\u001b[K\rremote: Compressing objects:  48% (22/45)\u001b[K\rremote: Compressing objects:  51% (23/45)\u001b[K\rremote: Compressing objects:  53% (24/45)\u001b[K\rremote: Compressing objects:  55% (25/45)\u001b[K\rremote: Compressing objects:  57% (26/45)\u001b[K\rremote: Compressing objects:  60% (27/45)\u001b[K\rremote: Compressing objects:  62% (28/45)\u001b[K\rremote: Compressing objects:  64% (29/45)\u001b[K\rremote: Compressing objects:  66% (30/45)\u001b[K\rremote: Compressing objects:  68% (31/45)\u001b[K\rremote: Compressing objects:  71% (32/45)\u001b[K\rremote: Compressing objects:  73% (33/45)\u001b[K\rremote: Compressing objects:  75% (34/45)\u001b[K\rremote: Compressing objects:  77% (35/45)\u001b[K\rremote: Compressing objects:  80% (36/45)\u001b[K\rremote: Compressing objects:  82% (37/45)\u001b[K\rremote: Compressing objects:  84% (38/45)\u001b[K\rremote: Compressing objects:  86% (39/45)\u001b[K\rremote: Compressing objects:  88% (40/45)\u001b[K\rremote: Compressing objects:  91% (41/45)\u001b[K\rremote: Compressing objects:  93% (42/45)\u001b[K\rremote: Compressing objects:  95% (43/45)\u001b[K\rremote: Compressing objects:  97% (44/45)\u001b[K\rremote: Compressing objects: 100% (45/45)\u001b[K\rremote: Compressing objects: 100% (45/45), done.\u001b[K\n",
            "Receiving objects:   0% (1/259)   \rReceiving objects:   1% (3/259)   \rReceiving objects:   2% (6/259)   \rReceiving objects:   3% (8/259)   \rReceiving objects:   4% (11/259)   \rReceiving objects:   5% (13/259)   \rReceiving objects:   6% (16/259)   \rReceiving objects:   7% (19/259)   \rReceiving objects:   8% (21/259)   \rReceiving objects:   9% (24/259)   \rReceiving objects:  10% (26/259)   \rReceiving objects:  11% (29/259)   \rReceiving objects:  12% (32/259)   \rReceiving objects:  13% (34/259)   \rReceiving objects:  14% (37/259)   \rReceiving objects:  15% (39/259)   \rReceiving objects:  16% (42/259)   \rReceiving objects:  17% (45/259)   \rReceiving objects:  18% (47/259)   \rReceiving objects:  19% (50/259)   \rReceiving objects:  20% (52/259)   \rReceiving objects:  21% (55/259)   \rReceiving objects:  22% (57/259)   \rReceiving objects:  23% (60/259)   \rReceiving objects:  24% (63/259)   \rReceiving objects:  25% (65/259)   \rReceiving objects:  26% (68/259)   \rReceiving objects:  27% (70/259)   \rReceiving objects:  28% (73/259)   \rReceiving objects:  29% (76/259)   \rReceiving objects:  30% (78/259)   \rReceiving objects:  31% (81/259)   \rReceiving objects:  32% (83/259)   \rReceiving objects:  33% (86/259)   \rReceiving objects:  34% (89/259)   \rReceiving objects:  35% (91/259)   \rReceiving objects:  36% (94/259)   \rReceiving objects:  37% (96/259)   \rReceiving objects:  38% (99/259)   \rReceiving objects:  39% (102/259)   \rReceiving objects:  40% (104/259)   \rReceiving objects:  41% (107/259)   \rReceiving objects:  42% (109/259)   \rReceiving objects:  43% (112/259)   \rReceiving objects:  44% (114/259)   \rReceiving objects:  45% (117/259)   \rReceiving objects:  46% (120/259)   \rReceiving objects:  47% (122/259)   \rReceiving objects:  48% (125/259)   \rReceiving objects:  49% (127/259)   \rReceiving objects:  50% (130/259)   \rReceiving objects:  51% (133/259)   \rReceiving objects:  52% (135/259)   \rReceiving objects:  53% (138/259)   \rReceiving objects:  54% (140/259)   \rReceiving objects:  55% (143/259)   \rReceiving objects:  56% (146/259)   \rReceiving objects:  57% (148/259)   \rReceiving objects:  58% (151/259)   \rReceiving objects:  59% (153/259)   \rReceiving objects:  60% (156/259)   \rReceiving objects:  61% (158/259)   \rReceiving objects:  62% (161/259)   \rReceiving objects:  63% (164/259)   \rReceiving objects:  64% (166/259)   \rReceiving objects:  65% (169/259)   \rReceiving objects:  66% (171/259)   \rReceiving objects:  67% (174/259)   \rReceiving objects:  68% (177/259)   \rReceiving objects:  69% (179/259)   \rReceiving objects:  70% (182/259)   \rReceiving objects:  71% (184/259)   \rremote: Total 259 (delta 28), reused 0 (delta 0), pack-reused 214\u001b[K\n",
            "Receiving objects:  72% (187/259)   \rReceiving objects:  73% (190/259)   \rReceiving objects:  74% (192/259)   \rReceiving objects:  75% (195/259)   \rReceiving objects:  76% (197/259)   \rReceiving objects:  77% (200/259)   \rReceiving objects:  78% (203/259)   \rReceiving objects:  79% (205/259)   \rReceiving objects:  80% (208/259)   \rReceiving objects:  81% (210/259)   \rReceiving objects:  82% (213/259)   \rReceiving objects:  83% (215/259)   \rReceiving objects:  84% (218/259)   \rReceiving objects:  85% (221/259)   \rReceiving objects:  86% (223/259)   \rReceiving objects:  87% (226/259)   \rReceiving objects:  88% (228/259)   \rReceiving objects:  89% (231/259)   \rReceiving objects:  90% (234/259)   \rReceiving objects:  91% (236/259)   \rReceiving objects:  92% (239/259)   \rReceiving objects:  93% (241/259)   \rReceiving objects:  94% (244/259)   \rReceiving objects:  95% (247/259)   \rReceiving objects:  96% (249/259)   \rReceiving objects:  97% (252/259)   \rReceiving objects:  98% (254/259)   \rReceiving objects:  99% (257/259)   \rReceiving objects: 100% (259/259)   \rReceiving objects: 100% (259/259), 3.87 MiB | 35.42 MiB/s, done.\n",
            "Resolving deltas:   0% (0/73)   \rResolving deltas:  30% (22/73)   \rResolving deltas:  31% (23/73)   \rResolving deltas:  32% (24/73)   \rResolving deltas:  41% (30/73)   \rResolving deltas:  42% (31/73)   \rResolving deltas:  43% (32/73)   \rResolving deltas:  46% (34/73)   \rResolving deltas:  47% (35/73)   \rResolving deltas:  52% (38/73)   \rResolving deltas:  54% (40/73)   \rResolving deltas:  57% (42/73)   \rResolving deltas:  58% (43/73)   \rResolving deltas:  60% (44/73)   \rResolving deltas:  73% (54/73)   \rResolving deltas:  87% (64/73)   \rResolving deltas:  90% (66/73)   \rResolving deltas:  98% (72/73)   \rResolving deltas: 100% (73/73)   \rResolving deltas: 100% (73/73), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy2vS167PVSg",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Final Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_gaW0COVMSQ",
        "colab_type": "code",
        "outputId": "4268d9c9-dee0-4d18-de06-03d2651cd3be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "\n",
        "# This algorithm is a simple command recognition algorithm designed to allow hobbyists and students to build a speech recognition\n",
        "# component into their own projects. This algorithm takes two parameters - a path to an audio file, and an audio file name - and\n",
        "# returns 1 of 11 classifications based upon the contents of the audio file. 10 of those classifications are simple speech \n",
        "# commands - 'yes','no','up','down','left','right','stop','go','on','off' - while the 11th classification is a catchall 'unknown'\n",
        "# category.\n",
        "\n",
        "# Valid audio files for this algorithm have a 16000hz sample rate and a maximum duration of 1 second. \n",
        "# Audio files are decoded into 16 bit samples.\n",
        "\n",
        "# The algorithm was trained and tested with 16 bit samples using the Speech Commands Dataset released by Google on August 3, 2017. The \n",
        "# data contains 64,727 one-second 16000hz audio clips of 30 short words. The audio files were crowdsourced by Google with the goal \n",
        "# of collecting single-word commands (rather than words as said and used in conversation). \n",
        "\n",
        "# RUN THIS CELL TO LOAD ALGORITHM INTO IPYTHON KERNEL\n",
        "\n",
        "import os\n",
        "from scipy.io import wavfile\n",
        "from scipy.fftpack import dct\n",
        "import numpy as np\n",
        "import keras\n",
        "import math\n",
        "from keras.models import load_model\n",
        "\n",
        "def speech_command_algo(audio_path, filename):\n",
        "    #print(\"My prediction for \"+filename+\" is...\")\n",
        "    model_path = os.path.join(os.path.dirname(os.path.abspath('__file__')),\n",
        "                              '/content/Simple-Audio-Recognition/Models',\n",
        "                              'model1_dr0.25_lr0.1_ra0.hdf5')  # путь к модели и модель\n",
        "    \n",
        "    \n",
        "    model = load_model(model_path)\n",
        "    mfccs = mfcc_conversion(audio_path, filename).reshape((1,79,12))\n",
        "    prediction = np.argmax(model.predict(mfccs))\n",
        "    print(\"My prediction for \"+filename+\" is...\") \n",
        "    if prediction == 1:\n",
        "        prediction = \"yes\"\n",
        "    elif prediction == 2:\n",
        "        prediction = \"no\"\n",
        "    elif prediction == 3:\n",
        "        prediction = \"up\"\n",
        "    elif prediction == 4:\n",
        "        prediction = \"down\"\n",
        "    elif prediction == 5:\n",
        "        prediction = \"left\"\n",
        "    elif prediction == 6:\n",
        "        prediction = \"right\"\n",
        "    elif prediction == 7:\n",
        "        prediction = \"on\"\n",
        "    elif prediction == 8:\n",
        "        prediction = \"off\"\n",
        "    elif prediction == 9:\n",
        "        prediction = \"stop\"\n",
        "    elif prediction == 10:\n",
        "        prediction = \"go\"\n",
        "    else:\n",
        "        prediction = \"UNKNOWN\"\n",
        "    print(prediction)\n",
        "    \n",
        "# given a filename, mfcc_conversion returns mel frequency cepstral coefficients array\n",
        "# mfcc_conversion returns array of (79,12) representing 79 audio frames described by 12 coefficients\n",
        "def mfcc_conversion(audio_path, filename, sample_rate = 16000, frame_size = 400, stride_size = 200, nfft = 512):\n",
        "    # decode audio\n",
        "    decoded_audio = audio_decoder(str(os.path.join(audio_path, filename)))    \n",
        "    audio = decoded_audio.reshape((16000,1))\n",
        "    \n",
        "    first_index = 0\n",
        "    mfcc_coefficients = np.empty((0,12))\n",
        "    # apply the following for each signal frame\n",
        "    while first_index <= audio.shape[0]-frame_size:\n",
        "        last_index = first_index+frame_size\n",
        "        frame = audio[first_index:last_index,:]\n",
        "        \n",
        "        # calculate discrete Fourier transform\n",
        "        frame = np.fft.fft(frame, n = nfft, axis=0)\n",
        "        \n",
        "        # calculate the periodogram estimate of the power spectrum; drop last half of values\n",
        "        power_spectrum = np.absolute(np.square(frame))/frame_size\n",
        "        power_spectrum = power_spectrum[0:int(nfft/2),:].astype(float)\n",
        "        power_spectrum = power_spectrum.reshape((power_spectrum.shape[0],))\n",
        "        # print(power_spectrum)\n",
        "        \n",
        "        # apply the mel filterbank to the power spectra, sum the energy in each filter\n",
        "            # frequencies on which to define mel filterbanks\n",
        "        mel_freqs = np.array([300, 383.4, 473.8, 571.7, 677.8, 792.7, 917.3, 1052.2, 1198.3, 1356.7, 1528.3,\n",
        "                              1714.2, 1915.6, 2133.7, 2370.1, 2626.3, 2903.7, 3204.4, 3530.1, 3882.9, 4265.2,\n",
        "                              4679.4, 5128.2, 5614.4, 6141.1, 6711.8, 7330.1, 8000]).astype(float)\n",
        "        vfunc = np.vectorize(bin_index)\n",
        "        mel_bins = vfunc(mel_freqs, nfft=nfft, sample_rate=16000)\n",
        "        # print(mel_bins)\n",
        "        \n",
        "            # calculate filterbank\n",
        "        mel_filterbank = np.empty((26,256))\n",
        "        for i in range(0,mel_filterbank.shape[0]):\n",
        "            for j in range(0,mel_filterbank.shape[1]):\n",
        "                mel_bin_min = mel_bins[i]\n",
        "                mel_bin_mid = mel_bins[i+1]\n",
        "                mel_bin_max = mel_bins[i+2]\n",
        "                if j >= mel_bin_min and j < mel_bin_mid:\n",
        "                    filter = float(j - mel_bin_min) / float(mel_bin_mid - mel_bin_min)\n",
        "                elif j >= mel_bin_mid and j < mel_bin_max:\n",
        "                    filter = float(mel_bin_max- j) / float(mel_bin_max - mel_bin_mid)\n",
        "                else:\n",
        "                    filter = float(0)\n",
        "                mel_filterbank[i,j] = filter\n",
        "                \n",
        "                # apply filterbank to power spectra and calculate log filterbank energies\n",
        "        logbankenergies = np.zeros((mel_filterbank.shape[0]))\n",
        "        for i in range(0, mel_filterbank.shape[0]):\n",
        "            mel_filters = mel_filterbank[i,:]\n",
        "            bankenergy = np.dot(power_spectrum,mel_filters)+1\n",
        "            logbankenergies[i] = np.log(bankenergy)\n",
        "            \n",
        "        \n",
        "        # take the discrete cosine transform of the log filterbank energies\n",
        "        log_dct = dct(logbankenergies)\n",
        "        \n",
        "        # saving DCT coefficients 2-13; discard rest\n",
        "        log_dct = np.transpose(log_dct[1:13])\n",
        "        i = first_index/stride_size\n",
        "        mfcc_coefficients = np.vstack((mfcc_coefficients, log_dct))\n",
        "                \n",
        "        # set up next frame\n",
        "        first_index = first_index + stride_size\n",
        "    \n",
        "    return mfcc_coefficients\n",
        "\n",
        "# calculates bin index given frequency and sample_rate\n",
        "def bin_index(frequency, nfft=512, sample_rate=16000):\n",
        "    bin = math.floor((nfft+1)*frequency/sample_rate)\n",
        "    return bin\n",
        "\n",
        "# decodes audio given a file name\n",
        "def audio_decoder(filename):\n",
        "    rate, data = wavfile.read(filename)\n",
        "    data = np.array(data)\n",
        "    if data.shape[0]<=16000:\n",
        "        difference = 16000 - data.shape[0]\n",
        "        data = np.append(data,np.zeros((difference,)))\n",
        "        data = np.transpose(data.reshape((16000,1)))\n",
        "        data = data.astype(int)\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIYtYp9w-b0A",
        "colab_type": "code",
        "outputId": "27678630-1281-48a5-f6fe-dfc3f8e9a088",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Загружаем фразу и путь к ней\n",
        "sample_phrase = '3_no_bed_bird.wav'\n",
        "dir_phrase = '/content/Simple-Audio-Recognition/Words_from_streams/no_bed_bird'\n",
        "\n",
        "dir_path = os.path.join(os.path.dirname(os.path.abspath('__file__')), dir_phrase)\n",
        "files = os.listdir(dir_path)\n",
        "sample_files = [i for i in files if i.endswith('.wav')]\n",
        "print(sample_files)\n",
        "\n",
        "for sample_file in sample_files:\n",
        "    speech_command_algo(dir_path,sample_file)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['1_no_bed_bird.wav', '3_no_bed_bird.wav', '2_no_bed_bird.wav']\n",
            "My prediction for 1_no_bed_bird.wav is...\n",
            "no\n",
            "My prediction for 3_no_bed_bird.wav is...\n",
            "UNKNOWN\n",
            "My prediction for 2_no_bed_bird.wav is...\n",
            "UNKNOWN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PvQOVdwMhJN",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ]
}